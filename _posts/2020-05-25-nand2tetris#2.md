---
layout: post
title: "Notes on nand2tetris #2"
comments: false
description: MOOC 
keywords: "Learn"
header-style: text
tags:
    - learning
    - series
---


We ended part 1 with the building out the hardware for our computer and also managed to write a low level language that can run instructions on top the our hardware, Now we move on to ze software (YAY) we start with the overview 

- Think of something cool 
- We write a program in a high level language 
- Compiler compiles high level code to an intermediatory VM code
- VM translator converts it to assembly code 
- Yoo now we can run this on our hardware

We missed the OS, this is gonna be used by our high level lang for doing stuff like graphics, IO from files, reading keyboard input, Storing objects, threads and so much more that we as programmers dont have to worry about... mostly


### Virtual Machines 

<http://otfried.org/courses/cs492fall2017/slides-vm-to-hack.pdf>

The problem with building compilers for high level language to machine language is that different machines have different architecture and hence would need a different compiler, which sucks.

> An alternative/ better approach is "Write one Run Anywhere"

For example Java compiles to Bytecode (VM code) that inturn runs with the help of JVM (java virtual machine) on different kinda hardware and ensures interoperability. This idea is called 2 tier compilation and was actually conceptualized by Turing

For our VM implementation we are gonna use the stack machine, which is nothing but an abstraction that consists of a stack and a set of operations we can apply to this architecture


<b> Stack Machine: </b>

A stack is a basic data structure that can be logically thought of as a linear structure represented by a real physical stack or pile, a structure where insertion and deletion of items takes place at one end called top of the stack and also stack pointer.

We have two major stack operations:
- Push x: Moves the value from memory location x to the stack pointer

- Pop y: Moved the value from the stack pointer to the memory location y

Using this we also perform arithmetic operations on the stack, like add and sub


| Stack => ADD  |      Stack  => NEG  | Stack |
|----------|:-------------:|------:|
| 12 |  12 |    12         |
| 3  |    10   |   -10 |
| 7 | | |


- Pop argument(s) from stack
- Compute f on the stack
- Pushed the result back to the stack

So these are all part of the abstraction and happen on the stack automatically

But But Why are we doing all this ? (We look at the big picture)

`x = 17 + 19` in a high level language gets compiled to 

```
push 17
push 19 
add 
pop x
```

Yay stack machine, here are the list of all the Arithmetic and Logical commands 

| Command 	| Return value 	| Return Type 	|
|---------	|--------------	|-------------	|
| Add        	|         x+y     	|     Integer        	|
| Sub     	| x-y             	| Integer            	|
| Neg        	|      -y        	|       Integer      	|
| eq        	|      x==0        	|       Boolean      	|
| gt        	|      x > y       	|       Boolean      	|
| lt        	|      y < x        	|       Boolean      	|
| and        	|      x and y        	|       Boolean      	|
| or        	|      x or y        	|       Boolean      	|
| not        	|      not y        	|       Boolean      	|

---
This is super powerful cause any arithmetic or logical operation can be expressed and evaluated by applying some sequence 
of the above operations on a stack 

---

This was demonstrated on stage by David Beazley the "Jimi Hendrix" of live coding <https://youtu.be/VUT386_GKI8?t=128>

<b> Memory Segments </b>


High level code has different types of variables, we can have static vars, local vars, arguments and so on. To preserve these roles we use memory segments which are just stacks and once we have these segments in place, the compiler can map the variables of the high level on these segments.

Syntax: `Push/Pop segment i` eg: push local 17

We have 8 mem segments: local, argument, this, that, constant, static, pointer, temp

> Note: You cannot pop a constant

local,argument,this,that: these method-level segments are mapped in an area called “heap”. The base addresses of these segments are kept in RAM addresses LCL, ARG, THIS, and THAT. Access to the i-th entry of any of these segments is implemented by accessing RAM[segmentBase + i]

Theory is all good but how do achieve this, so let's refresh pointers 

| RAM 	| Memory |
|---------	|
| 22        | 256 
| 31     	| 257
| 200       | 258
| 28       	| 259

```
p1 = 256
p1 = p1 + 3
*p1 = *p1 + 3
p2 = p1 - 2
p1--
*(p2 + 1) = *p1 + *p2              
```

What will be the value of RAM[258] following the code execution?

```
p1 = 256 // p1 = 256
p1 = 259 // p1 = p1 + 3
RAM[259] = 31 // *p1 = *p1 + 3
p2 = 257 // p2 = p1 - 2
p1 = 258 // p1--
RAM[258] = RAM[257] + RAM[258] // *p1 + *p2 
=> 231
```

So now to push something to a stack we have to do two things, assign the pushed value to SP's memory location following which we increment the stack pointer.

```
# VM CODE 
PUSH CONSTANT 17 
```
|| VM Translator || 

```
# Assembly
*SP = 17
SP++
```

A VM translator is a program that translates VM code to machine language

In our RAM we have a stack pointer and a Base Address that points to a memory segment depending on the type of segment

for example if our base address is 5 for temp memory segment, then

`push temp i` =>  `addr = 5+i, *SP = *addr, SP++`

`pop temp i` =>  `addr = 5+i, SP-- , *addr = *SP`


`This` and `That` memory segments represent the current object and the current array that the method in a high level language maybe processing. We keep the base addresses of this and that in the pointer memory segment

`Push/ Pop pointer 0/1` pointer can be either 0 or 1 (A fixed 1 place segment)

- Accessing 0 should result in accessing THIS

- Accessing 1 should result in accessing THAT

<b> Branching </b>

Now we come to functions and loops, two very powerful high level programming concepts

Todo understand this a low level we divide branching into conditonal and unconditional

- goto label // Jump to execute command just after label

- if-goto label // cond = pop; if cond jump to execute command just after label

- label label // label declaration


With our VM language we have two types of functions

- Primitive Functions like `add` and `sub` that are already defined, so we can just call `add`

- Abstract Functions that we call from somewhere, In our case we call the OS for math.multiply with `call Math.multiply` these have the same look and feel of a primitive function and makes our vm lang extensible

<https://www.coursera.org/learn/nand2tetris2/lecture/R5080/unit-2-3-functions-abstraction> 12:10

![img](https://i.ibb.co/KwNk2ys/functions-in-vm.png)

We `call mult 2` to denote the number of arguments and `function mult 2` to represent the number of local vars. Return pushes the topmost local var back to the stack before popping the arguments

Every VM function has a private set of 5 memory segments (local, argument, this, that, pointer) These resources exist as long
as the function is running.

### Implementing function call and return

Functions have callers and returns, callers can be chained for eg: a function can call another function which can call call another function and so on, the return of the previous function goes in as the argument. These recursive calling and returning is a LIFO activity and so are such a good usecase for stacks. Once the functions runs and returns its local values are garbage collected to keep a clean slate and if the function used too many local vars in in memory such that the stack can't handle any more recursive function we get an overflow error, in any high level lang. Functions always returns something even void functions (Push something to the stack), the caller can choose to ignore these like in the case of void functions. 


![img](https://i.imgur.com/RpOSMeal.png)


### High Level Language

So in this module we learn about jack a general object oriented high level language based on java, It is almost a refreshed on every GENERIC_LANGUAGE101 course I took in college. Jack works with classes, there is a Main class that has a main() method which is where the execution of every jack program begins. We can instantiate an instance of a class as an object using a constructor, call methods defined in the class using the object and maybe even pass it to functions defined outside the class. 

Suppose we have an class `c = Test()` and we call this method `c.cry()` now this method takes this object as a parameter into its execution but it has no context on c so it gets around this by using the keyword `this`  which will always represent the current object being operated to be used by methods and constructors

The Jack language operates on basic data types like int, float, char etc and even a collection of data types like a list, list is defined as an atom or null followed by the list (it do be highly recursive ngl)

```
null                = ()
(5,null)            = (5)
(3,(5,null))        = (3,5)
(2,3(3,(5,null)))   = (2,3,5)
```
List is a single object of linked memory resources (Linked list)

Jack relies on OS to expose much of its APIs just like other languages, for instance Math is can be class we import and use which is actually exposed by the OS, we also have other interactions like reading from the keyboard and writing to the display. Also jack is not a strongly typed language and has methods that can directly alter memory addresses in the RAM (lil dangerous)


### Compiler Time

This module are gonna build a compiler that translates our high level language to a low level language, this process is made of two well-defined and more or less independent stages, syntax analysis, and code generation. 


**Syntax Analysis**:

In most modern programming languages, we follow the two tier compilation. For instance in Java the high level code is compiled to Bytecode (In our case VM code) which is then VM translator converts to machine code that actually runs and does all the magix. 

The syntax analyzer is made up of two components, a tokenizer and a parser, we are gonna start with the tokenizer or the lexical analyser which considers any program you write as just a stream of characters.
```bash
if (x>0) {  
    // some useless comment                   
    let sign = "negative";  
}
tokenizing
===============> <start> if ( x < 0 ) { let sign = "negative" ; } <end>
```

So it groups the code to the stream of tokens, removing whitespace comments and keeping only what's necessary. Tokens are a string of characters that have meaning. For example when you have `x++` in may an incremenent in some language or raise an exception in other, A tokenizer must be aware of this and must document allowable tokens in its specification. In jack we have 5 tokens

- Keywords `(Class | constructor | function | method | char | let... etc)`
- Symbols `{ | } | & | = | ~ | ; | +... etc`
- Integers `A decimal in range 0 32767`
- Strings `A seq of unicode characters including double quotes or newline` 
- Identifiers `seq of alphanumeric characters or an underscore character used to represent var, funcs, arrays , unions etc, identifiers are user defined`

Tokenizer can use this knowledge to handle our programs stream of tokens.

eg: When we pass `let sign = "negative";`  to our tokenizer we will get

```xml
</keyword> let </keyword>
</identifier> sign  </identifier>
</symbol> =  </symbol>
</stringConstant> negative  </stringConstant>
</symbol> ;  </symbol>
```

Now we have tokens, that does not we know what those means. The order of tokens are rather the *Grammar* is really important to understand how tokens can be combined to create valid language contructs 

Similar to english grammar we have jack grammar (`*` means one or more and `?` is optional)

![img](https://i.imgur.com/ulYS2YQ.png)

for example letStatement  `varName '=' expression ';' `  covers `let x = 100;` 
and when the grammar does not match we throw a `SyntaxError`

Note: `let x = y = 5;` is also valid since "y = 5" is interpreted as an expression

When you deal with a compiler, the input must match the grammar perfectly. And if there's any discrepancy, the compiler will raise its hands and say, we have a syntax error and you have to fix your program.

After making sure that our tokens are sane we conform it to a parse tree, for example the statement `count+1`
parses to a tree that looks kinda like this 
 
``` 
			  |=======> term ====> count
expression----|==================> +
			  |=======> term ====> 1
```

The same parse tree can be represented in XML


```xml
<expression>
	<term>
		<identifier> count </identifier>
	</term>
	<symbol> + </symbol>
	<term>
		<integerConstant> 1 </integerConstant>
	</term>
</expression>
```

Now how do I construct this parse tree ? how to get from `count + 1` to this xml file 

```java
class CompilationEngine{
// Parses the tokens using these methods (rules) to an XML file 
	compileStatements(){
	/*
	One or more statement
	ifStatement | letStatement | whileStatement 

	letStatement: 'let' varName'='expression';'

	expression: term(op term)?
	expression is a term with an optional op followed by term

	op: '+' | '-' | '*' | '=' | '>' | '<'
	*/ 
	}
	compileIfStatement(){
	/* 
	'if' '(' expression ')'
	'{' statements '}' 
	*/
	}
	compileWhileStatement(){
	/* 
	'while' '(' expression ')'
	'{' statements '}' 
	*/
	}
	compileTerm(){
	/* term is varName | constant
	varName: string not beginning with a digit
	constant: a decimal number
	*/
	}
}
```

What we have build here is an LL(1) parser

**LL grammar** : can be parsed by a recursive descent parser without backtracking

**LL(k) grammar** : the parser need to look ahead at most k tokens in order to determine which rule is applicable 

Jack is LL(1) since when we look ahead 1 token and see an `if` or a `while` and then jump, but when it comes to identifiers and only in that case jack is LL(2) since after we have foo, it can be just foo or maybe an array foo[something] or a method foo.caller(), so we need to look 2 tokens ahead. So we need to evaluate expressions a little carefully while parsing our high level code

We have two types of rules in our language, first are the terminal rules with keywords or symbols, once you see them you terminate your rule eg: `<symbol> + </symbol>` or `<keyword> class </keyword>` 

We also have non terminating rules which are of two types. The first type includes class declaration, subroutine, if, while and other branching conditions, an expression, term or a list of expressions and so on. So these rules are basically a recursive combination of existing rules and in this case our parser does something like this...

```xml
<nonterminal>
	Recursive output for the non terminal body
</nonterminal>
```

Now we also have another type of non terminal rule which does not even make it into the xml for eg: if we have `'let' varname '=' expression ';'` then we don't have a <varname> tag but rather use <identifier> so varname here is rather insignificant or implicit. 

Now we are offcially done with the first half of our compiler, the Tokenizer and the Parser.