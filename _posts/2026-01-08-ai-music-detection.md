---
layout: post
title: "AI music detection with checkerboard artifacts"
comments: false
keywords: "Learn"
tags:
    - music-tech
---


Last year was crazy for AI music tools; the heavily funded ones ended up getting record deals, and their usage has skyrocketed. There is a ton of AI slop everywhere now, and it's no different with music. Apps like Spotify have already been pushing AI music to their listeners for years, and it's only going to get worse. I'm not writing to talk about that, though; I want to talk about two things: How do we reliably detect AI music, and how do we protect artists and their music from this regurgitated, algorithmic hell-slop?


Several independent researchers have developed models for detecting and tagging AI music, which, to be honest, is a great step in the right direction. But the industry keeps evolving, and models can always be engineered to beat this metaphorical Turing test. There are always issues with interpretability and generalization of these AI detection models.


There are also tools like [HarmonyCloak](https://mosis.eecs.utk.edu/harmonycloak.html) that claim to make music unlearnable for generative AI models while leaving no perceptible artifacts, but I've talked to a few friends who refute this claim and say this is just the beginning of something—not really a practical model. HarmonyCloak seems to have started with the MIDI format and does not yet support raw audio formats, and the paper just mentions, "The raw audio waveform format can be either converted from the MIDI files using DAWs or recorded from physical instruments." Bruh, what?! And this might be a nitpick, but I was never able to run the model locally because of the massive compute required to add imperceptible adversarial noise. It's not all bad, though, because there is potential work yet to be done in this research direction.

![ai-ai-thanos](/img/in-post/ai-0music.jpeg)


Benn Jordan made a couple of cool videos about [detecting AI music](https://www.youtube.com/watch?v=QVXfcIb3OKo&t) and [poison-pilling music](https://www.youtube.com/watch?v=xMYm2d9bmEA&t=358s) to make it unlearnable with adversarial noise, kind of the same direction: using AI to beat AI. Very related, but I'm also plugging Benn because I enjoy his videos.


The most interesting work yet has been from Deezer, which is also ISMIR 2025's best paper: **A Fourier Explanation of AI-Music Artifacts**. It's elegant in a way that I was able to reproduce some of the work in the paper so easily and locally with just my Jupyter notebook, which, these days, is so hard to do without a ton of GPUs at your disposal.

### Checkerboard artifact


There was a paper by Google way back in 2016 talking about [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)


A lot of generative tools use deconvolution layers for upsampling the latent representation, and when doing so, pixels sometimes receive more "contributions" from the convolution than others, creating intensity variations in a regular pattern.


**For audio, this means that the outputs of generative models will produce small frequency artifacts, i.e., peaks.**


This depends on the model architecture, because models can use modified upsampling mechanisms to avoid the irregular kernel overlap.


I am calculating a basic STFT to convert the audio to the frequency domain, taking the average spectrum over time with a sliding window and detecting peak values. We subtract the local minima of the spectrum over sliding windows to highlight the local variations and also remove bandwidth that is of no use (filtering [5-16 kHz]).


This is how the plot looks for a track generated by Suno:

![ai](/img/in-post/ai-fourier.png)


And this is how it looks for "Everything in Its Right Place" by Radiohead:

![ai](/img/in-post/not-ai-fourier.png)


You can see a very clear difference. For the real track, the peaks are relatively uniform and scattered throughout the frequency range; the pattern looks more like natural harmonic content. AI music has a much sharper frequency cutoff and peaks that are more periodic and really stand out.


Once we have these so-called fingerprints, we can just train a very simple linear classifier, since different models have different peak patterns.


This idea is also not robust, since simple audio manipulations like pitch shifting can change the peak frequencies and easily break detection. In the future, models could have different upsampling techniques to make these artifacts disappear.


But it demonstrates something very cool: **AI music artifacts are not bugs but mathematical certainties given the architecture choice**, and you don't need millions of parameters to detect AI music—sometimes an STFT will do :)